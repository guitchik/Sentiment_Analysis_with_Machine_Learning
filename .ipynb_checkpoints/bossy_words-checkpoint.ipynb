{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; print(os.path.dirname(os.getcwd()).split('\\\\')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Who are the Bossy Words?\n",
    "\n",
    " On this activity you will use TF-IDF to find the most relevant words on news articles that talk about money in the [Reuters Corpus](https://www.nltk.org/book/ch02.html#reuters-corpus) bundled in `NLTK`. Once you find the most relevant words, you should create a word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial imports\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "mpl.rcParams[\"figure.figsize\"] = [20.0, 10.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Loading the Reuters Corpus\n",
    "\n",
    " The first step is to load the Reuters Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download/update the Reuters dataset\n",
    "nltk.download(\"reuters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Getting the News About Money\n",
    "\n",
    " You will analyze only news that talk about _money_. There are two categories on the Reuters Corpus that talk about money: `money-fx` and `money-supply`. In this section, you will filter the news by these categories.\n",
    "\n",
    " Take a look into the [Reuters Corpus documentation](https://www.nltk.org/book/ch02.html#reuters-corpus) and check how you can retrieve the categories of a document using the `reuters.categories()` method; write some lines of code to retrieve all the news articles that are under the `money-fx` or the `money-supply` categories.\n",
    "\n",
    " **Hint:**\n",
    " You can use a comprehension list or a for-loop to accomplish this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Comment</font>\n",
    "\n",
    "Notice how the following list comprehension is multi-line. Easier?\n",
    "\n",
    "Q) What is `categories[1]`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Getting all documents ids under the money-fx and money-supply categories\n",
    "categories = [\"money-fx\", \"money-supply\"]\n",
    "all_docs_id = reuters.fileids()\n",
    "money_news_ids = [\n",
    "    doc\n",
    "    for doc in all_docs_id\n",
    "    if categories[0] in reuters.categories(doc)\n",
    "    or categories[1] in reuters.categories(doc)\n",
    "]\n",
    "\n",
    "print(f\"Total number of news articles about money: {len(money_news_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Comment</font>\n",
    "\n",
    "The list comprehension above and below could be merged into one mega line. But it's so unreadable!\n",
    "\n",
    "```python\n",
    "money_news_ids = [reuters.raw(doc).lower() for doc in all_docs_id if categories[0] in reuters.categories(doc) or categories[1] in reuters.categories(doc)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the working corpus containing the text from all the news articles about money\n",
    "money_news = [reuters.raw(doc).lower() for doc in money_news_ids]\n",
    "\n",
    "# Printing a sample article\n",
    "print(money_news[78])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Calculating the TF-IDF Weights\n",
    "\n",
    " Calculate the TF-IDF weight for each word on the working corpus using the `TfidfVectorizer()` class. Remember to include the `stop_words='english'` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating TF-IDF for the working corpus.\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(money_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a DataFrame representation of the TF-IDF weights of each term in the working corpus. Use the `sum(axis=0)` method to calculate a measure similar to the term frequency based on the TF-IDF weight, this value will be used to rank the terms for the word cloud creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame Representation of the TF-IDF results\n",
    "money_news_df = pd.DataFrame(\n",
    "    list(zip(vectorizer.get_feature_names(), np.ravel(X.sum(axis=0)))),\n",
    "    columns=[\"Word\", \"Frequency\"],\n",
    ")\n",
    "\n",
    "# Order the DataFrame by word frequency in descending order\n",
    "money_news_df = money_news_df.sort_values(by=[\"Frequency\"], ascending=False)\n",
    "\n",
    "# Print the top 10 words\n",
    "money_news_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Retrieving the Top Words\n",
    "\n",
    " In order to create the word cloud you should get the top words, in this case we will use a thumb rule that has been empirically tested by some NLP experts that states that words with a frequency between 10 and 30 might be the most relevant in a corpus.\n",
    "\n",
    " Following this rule, create a new DataFrame containing only those words with the mentioned frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = money_news_df[money_news_df[\"Frequency\"] >= 10]\n",
    "\n",
    "top_words.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top words will be those with a frequency between 10 ans 30 (thumb rule)\n",
    "top_words = money_news_df[\n",
    "    (money_news_df[\"Frequency\"] >= 10) & (money_news_df[\"Frequency\"] <= 30)\n",
    "]\n",
    "\n",
    "top_words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Creating Word Cloud\n",
    "\n",
    " Now you have all the pieces needed to create a word cloud based on TF-IDF weights, so use the `WordCloud` library to create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a string list of terms to generate the word cloud\n",
    "terms_list = str(top_words[\"Word\"].tolist())\n",
    "\n",
    "# Create the word cloud\n",
    "wordcloud = WordCloud(colormap=\"RdYlBu\").generate(terms_list)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "fontdict = {\"fontsize\": 20, \"fontweight\": \"bold\"}\n",
    "plt.title(\"Money News Word Cloud\", fontdict=fontdict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Looking for Documents that Contains Top Words\n",
    "\n",
    " Finally you might find interesting to search those articles that contain the most relevant words. Create a function called `retrieve_docs(terms)` that receive a list of terms as parameter and extract from the working corpus all those news articles that contains the search terms. On this function you should use the `reuters.words()` method to retrieve the tokenized version of each article as can be seen on the [Reuters Corpus documentation](https://www.nltk.org/book/ch02.html#reuters-corpus).\n",
    "\n",
    " **Hint:** To find any occurrence of the search terms you might find quite useful [this post on StackOverflow](https://stackoverflow.com/a/25102099/4325668), also you should lower case all the words to ease your terms search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Comment</font>\n",
    "\n",
    "Explore how this upcoming function works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [\"yen\", \"japan\"]\n",
    "\n",
    "# print(money_news_ids[0:2])\n",
    "for doc_id in money_news_ids[0:2]:\n",
    "    print(doc_id)\n",
    "    \n",
    "    for word in reuters.words(doc_id)[0:5]:\n",
    "        print(word)\n",
    "        \n",
    "        if any(term in word.lower() for term in terms):\n",
    "            print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(terms):\n",
    "    result_docs = []\n",
    "    for doc_id in money_news_ids:\n",
    "        found_terms = [\n",
    "            word\n",
    "            for word in reuters.words(doc_id)\n",
    "            if any(term in word.lower() for term in terms)\n",
    "        ]\n",
    "        if len(found_terms) > 0:\n",
    "            result_docs.append(doc_id)\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Question 1: How many articles talk about Yen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(retrieve_docs([\"yen\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: How many articles talk about Japan or Banks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(retrieve_docs([\"japan\", \"banks\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Question 3: How many articles talk about England or Dealers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(retrieve_docs([\"england\", \"dealers\"]))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
